{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel0 \b \fs36 Mainpipe Pipeline Report\par}
{\pard \ql \f0 \sa180 \li0 \fi0 {\b Prepared By:} Sally Lan\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel1 \b \fs32 Table of Contents\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab {\field{\*\fldinst{HYPERLINK "#executive-summary"}}{\fldrslt{\ul
Executive Summary
}}}
\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab {\field{\*\fldinst{HYPERLINK "#stage-1--data-acquisition--language-filtering"}}{\fldrslt{\ul
Stage 1 \u8212- Data acquisition + language filtering
}}}
\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab {\field{\*\fldinst{HYPERLINK "#stage-2--text-cleaning--normalisation"}}{\fldrslt{\ul
Stage 2 \u8212- Text cleaning & normalisation
}}}
\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab {\field{\*\fldinst{HYPERLINK "#stage-3--safety-filtering-pii--toxicity"}}{\fldrslt{\ul
Stage 3 \u8212- Safety filtering (PII & toxicity)
}}}
\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab {\field{\*\fldinst{HYPERLINK "#stage-4--deduplication"}}{\fldrslt{\ul
Stage 4 \u8212- Deduplication
}}}
\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab {\field{\*\fldinst{HYPERLINK "#stage-5--tokenisation"}}{\fldrslt{\ul
Stage 5 \u8212- Tokenisation
}}}
\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab {\field{\*\fldinst{HYPERLINK "#stage-51--compute-perplexity"}}{\fldrslt{\ul
Stage 5.1 \u8212- Compute perplexity
}}}
\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab {\field{\*\fldinst{HYPERLINK "#stage-6--inspectability--visualization-analysis"}}{\fldrslt{\ul
Stage 6 \u8212- Inspectability & Visualization Analysis
}}}
\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab {\field{\*\fldinst{HYPERLINK "#stage-7--conceptual-plan-for-scaling"}}{\fldrslt{\ul
Stage 7 \u8212- Conceptual plan for scaling
}}}
\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab {\field{\*\fldinst{HYPERLINK "#final-section--main-takeaways-of-performance"}}{\fldrslt{\ul
Final Section \u8212- Main Takeaways of Performance
}}}
\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab {\field{\*\fldinst{HYPERLINK "#appendix--visualization-insights"}}{\fldrslt{\ul
Appendix \u8212- Visualization Insights
}}}
\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel1 \b \fs32 Executive Summary\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Purpose\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Transform multi-language raw text into safe, deduplicated, tokenized English corpora suitable for LLM fine-tuning.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Key outcomes\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab 183,052 kept samples; safety filters removed 7,379 risky items; MinHash dedup trimmed 19.7% duplicates without quality loss.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 High-level metrics\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Average kept sample = 1,414 chars / 312 tokens.\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab PII hit rate 2.7% and toxicity rate 0.41%.\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Perplexity mean 41.8 (median 30.6).\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Throughput peaks at 9,081 samples/sec during tokenization.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel1 \b \fs32 Stage 1 \u8212- Data acquisition + language filtering\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Method used\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab {\f1 pipeline.loader.load_jsonl} plus {\f1 langdetect} enforcing {\f1 target_language=en}.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Why\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Maintain a monolingual corpus to prevent context mixing that destabilizes downstream instruction-following.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Results / metrics\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab 244,559 language-vetted samples loaded.\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab English represents 90.8% of detections with German (3.8%) and Spanish (3.2%) as the largest stragglers, highlighting stable upstream sourcing.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Limitations\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab {\f1 langdetect} exposes only hard labels, preventing confidence-aware triage for borderline sentences.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Future improvements\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Replace detector with FastText or langid to unlock the planned Tier 2 \u8220"summarize + re-evaluate\u8221" workflow.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Implementation & insight\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Language mix skew (long English tail with minor European languages) indicates crawl consistency but also shows potential for optional multilingual branches without retooling loaders.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel1 \b \fs32 Stage 2 \u8212- Text cleaning & normalisation\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Method used\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab {\f1 pipeline.cleaner.clean_samples} applying HTML stripping, whitespace normalization, and character-length bounds of 50\u8211-100k.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Why\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Remove boilerplate markup, normalize formatting, and constrain documents to sizes that tokenize reliably.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Results / metrics\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab 7,431 documents dropped (99.7% due to being <50 chars), leaving 237,128 samples\u8212-evidence that most raw inputs already provide sufficient context length.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Limitations\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Binary cutoffs discard concise but high-signal snippets and do not split ultra-long documents into shards.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Future improvements\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Add adaptive chunking for long content and summarization for short-but-useful notes.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Implementation & insight\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Cleaning drop distribution (short > long by 322\u215 ?) shows upstream scraping already curbs web noise; tuning the minimum threshold could rescue niche datasets without affecting quality.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel1 \b \fs32 Stage 3 \u8212- Safety filtering (PII & toxicity)\par}
{\pard \qc \f0 \sa180 \li0 \fi0 \emdash\emdash\emdash\emdash\emdash\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Method used\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Deterministic heuristic toxicity scoring (blocklists + uppercase/symbol/repetition penalties + obfuscated-profanity regex) paired with regex-based PII detection for EMAIL, PHONE, CREDIT_CARD, SSN, IP.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Why\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Guarantee explainable, reproducible safety decisions without GPU inference dependencies.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Results / metrics\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab 237,128 entries scanned (768-char window).\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Heuristics flagged 971 toxic samples (0.41%).\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab PII detectors removed 6,408 records (2.7%), demonstrating that privacy risks dominate the safety tail.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Limitations\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Rule sets miss nuanced harassment and subtle PII formats; regex false positives (e.g., code blocks) still waste reviewer time.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Future improvements\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Layer detoxify-like ML scorers and NER-driven PII detectors with confidence thresholds to reduce both misses and false alarms.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Implementation & insight\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Phone (2,984 hits) and email (2,844 hits) patterns account for 89% of PII drops, while SSNs are rare (7 hits); PII hits overall are 6.6\u215 ? more frequent than toxicity hits, so the next investment should target privacy detection breadth before advanced toxicity modeling.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel1 \b \fs32 Stage 4 \u8212- Deduplication\par}
{\pard \qc \f0 \sa180 \li0 \fi0 \emdash\emdash\emdash\emdash\emdash\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Method used\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab MinHash LSH (128 permutations, 0.9 Jaccard threshold) via {\f1 pipeline.deduplicator.deduplicate}.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Why\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Remove repeated documents to minimize memorization and licensing exposure.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Results / metrics\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab 45,183 duplicates removed (19.7% duplicate rate), leaving ~184k unique entries\u8212-indicative of partially overlapping crawls.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Limitations\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab LSH accuracy drops for short samples and scales poorly with corpus size, risking slowdowns at >1M documents.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Future improvements\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Introduce streaming dedup with hashed fingerprints or partitioned shingles to bound memory and CPU.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Implementation & insight\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Duplicate share closely mirrors historic web crawl overlap, so upstream dedup (per domain) could reclaim ~20% pipeline time before data even enters MinHash.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel1 \b \fs32 Stage 5 \u8212- Tokenisation\par}
{\pard \qc \f0 \sa180 \li0 \fi0 \emdash\emdash\emdash\emdash\emdash\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Method used\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab {\f1 pipeline.tokenizer.tokenize_samples} with tiktoken {\f1 cl100k_base}, enforcing 10\u8211-8,192 token bounds.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Why\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Align dataset with typical OpenAI context windows and avoid fragments that fail to train sequence models.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Results / metrics\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Average kept sample = 312 tokens (median 168).\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab 1,514 records dropped (1,000 too short, 514 too long) leaving 183,052 training-ready rows.\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Length histogram skews toward 150\u8211-400 tokens, validating the cleaning thresholds.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Limitations\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Uniform token limits ignore downstream model variation (4k vs 16k contexts) and do not create curriculum bins.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Future improvements\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Parameterize token bounds per export profile and optionally emit stratified shards.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Implementation & insight\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab The token-length hump around 200 tokens suggests we can raise the minimum from 10 to ~25 without losing coverage, improving effective context per sample.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel1 \b \fs32 Stage 5.1 \u8212- Compute perplexity\par}
{\pard \qc \f0 \sa180 \li0 \fi0 \emdash\emdash\emdash\emdash\emdash\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Method used\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab {\f1 pipeline.perplexity.compute_perplexity} sampling 1,000 kept documents and running GPT-2 small inference.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Why\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Track linguistic quality and detect drift in a lightweight, repeatable manner.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Results / metrics\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Mean perplexity 41.8 (median 30.6, std 50.7, min 2.2, max 949).\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Long upper tail indicates sparse low-quality spans despite strong median quality.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Limitations\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Single English GPT-2 checkpoint underrepresents domain-specific or multilingual issues; the 1k sample may miss rare regressions.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Future improvements\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Add domain-tuned checkpoints, dynamic sample sizing, and percentile-based gating before export.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Implementation & insight\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab 75% of evaluated docs fall below perplexity 47.9, so gating at 60 would catch only anomalous spikes while leaving the main distribution untouched.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel1 \b \fs32 Stage 6 \u8212- Inspectability & Visualization Analysis\par}
{\pard \qc \f0 \sa180 \li0 \fi0 \emdash\emdash\emdash\emdash\emdash\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Method used\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab {\f1 pipeline.inspector.inspect} compiles JSON/CSV summaries and renders Matplotlib charts for token lengths, safety drops, PII types, throughput, and language mix.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Why\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Give reviewers a dashboard-equivalent artifact directly in the repo.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Results / metrics\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Token-length plot confirms the 200-token mode.\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Safety charts show PII removal dominating toxicity.\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Throughput visualization highlights load/dedup bottlenecks (each under 650 samples/sec).\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Limitations\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Static PNGs require reruns for new slices, and there is no drill-down into sample-level evidence.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Future improvements\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Pipe metrics into DuckDB/Parquet and layer an interactive notebook or dashboard for slice-and-dice QA.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Implementation & insight\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Visual overlays make it obvious that dedup time dwarfs other stages despite modest drop counts, reinforcing the need for architectural change rather than mere parameter tuning.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel1 \b \fs32 Stage 7 \u8212- Conceptual plan for scaling\par}
{\pard \qc \f0 \sa180 \li0 \fi0 \emdash\emdash\emdash\emdash\emdash\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Method used / architecture choices\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Near-term multiprocessing and streaming; mid-term Ray dataflow with cloud storage; long-term Spark-style distributed tokenization plus checkpointed incremental runs.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Why\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Current bottlenecks (load at 492 samples/sec, dedup at 616 samples/sec, perplexity at 3 samples/sec) limit throughput as corpora grow toward billions of tokens.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Results / metrics (expected)\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Ray should sustain \u8805 ?2,500 samples/sec per worker for language + safety.\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Parquet/DuckDB export minimizes downstream I/O.\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Spark integration targets petabyte-scale ingestion with fault tolerance.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Limitations / risks\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Distributed dedup complicates coordination; GPU tokenization requires specialized hardware; remote storage adds auth and latency failure modes.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Future improvements / migration plan\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Pilot Ray for language/safety, adopt S3/GCS streaming connectors, then roll Spark once sharding and checkpointing stabilize.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Monitoring & failure modes\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Continue per-stage timers ({\f1 throughput_metrics.stages}), add executor health probes, and alert on slow shards or stalled checkpoints.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel1 \b \fs32 Final Section \u8212- Main Takeaways of Performance\par}
{\pard \qc \f0 \sa180 \li0 \fi0 \emdash\emdash\emdash\emdash\emdash\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Top performance highlights\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Cleaning + tokenization sustain multi-thousand samples/sec.\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Deterministic safety maintains reviewer trust.\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Inspector artifacts provide immediate QA context without external dashboards.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Key bottlenecks identified\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab JSONL loading and MinHash dedup dominate wall-clock time.\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Perplexity inference lags at 3.1 samples/sec.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Priority next steps\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Parallelize load/dedup via Ray or Spark.\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Adopt probability-aware language detection.\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Layer ML safety assistance.\par}
{\pard \ql \f0 \sa0 \li360 \fi-360 \bullet \tx360\tab Persist metrics in DuckDB/Parquet to enable interactive QA and alerting.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel1 \b \fs32 Appendix \u8212- Visualization Insights\par}
{\pard \qc \f0 \sa180 \li0 \fi0 \emdash\emdash\emdash\emdash\emdash\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Char Length Histogram\par}
{\pard \ql \f0 \sa180 \li360 \fi-360 \bullet \tx360\tab Most kept samples fall between 500 and 2,000 characters, validating the 50\u8211-100k cleaning bounds.\par}
{\pard \ql \f0 \sa180 \li360 \fi-360 \bullet \tx360\tab Heavy density in this band confirms documents retain substantial narrative context for training.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Token Length Histogram\par}
{\pard \ql \f0 \sa180 \li360 \fi-360 \bullet \tx360\tab Peak near 200 tokens mirrors the character histogram, showing mid-length contexts dominate.\par}
{\pard \ql \f0 \sa180 \li360 \fi-360 \bullet \tx360\tab Supports raising the minimum token threshold (e.g., from 10 to ~25) without losing coverage.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Cleaning Drop Reasons\par}
{\pard \ql \f0 \sa180 \li360 \fi-360 \bullet \tx360\tab \u8220"Too short\u8221" removals dwarf \u8220"too long,\u8221" proving upstream scraping already curbs oversized docs.\par}
{\pard \ql \f0 \sa180 \li360 \fi-360 \bullet \tx360\tab Residual short snippets highlight an opportunity for summarization instead of full drops.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Language Distribution\par}
{\pard \ql \f0 \sa180 \li360 \fi-360 \bullet \tx360\tab English holds a dominant share with small German/Spanish/French slices, matching loader stats.\par}
{\pard \ql \f0 \sa180 \li360 \fi-360 \bullet \tx360\tab The long tail suggests we can optionally branch multilingual processing without retooling ingestion.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 PII Removals\par}
{\pard \ql \f0 \sa180 \li360 \fi-360 \bullet \tx360\tab Phone numbers (2,984) and email addresses (2,844) represent ~89% of PII hits.\par}
{\pard \ql \f0 \sa180 \li360 \fi-360 \bullet \tx360\tab Aligns with prioritizing communication-identifier detection before rarer entities like SSNs (7 hits).\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Toxic Removals\par}
{\pard \ql \f0 \sa180 \li0 \fi0 {\cf1 [image: visualizations/toxic_removals.png]\cf0}\par}
{\pard \ql \f0 \sa180 \li360 \fi-360 \bullet \tx360\tab Toxicity drops are comparatively low but show noticeable spikes tied to uppercase/obfuscation-heavy texts.\par}
{\pard \ql \f0 \sa180 \li360 \fi-360 \bullet \tx360\tab Confirms deterministic heuristics focus on high-confidence rule violations.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Perplexity Distribution\par}
{\pard \ql \f0 \sa180 \li360 \fi-360 \bullet \tx360\tab Median lies near 30 with a long tail extending beyond 900, evidencing sparse low-quality spans.\par}
{\pard \ql \f0 \sa180 \li360 \fi-360 \bullet \tx360\tab Supports gating exports near perplexity 60 to capture outliers without affecting the bulk.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Throughput Overview\par}
{\pard \ql \f0 \sa180 \li360 \fi-360 \bullet \tx360\tab {\f1 load_jsonl} (~492 samples/sec) and deduplication (~616 samples/sec) bars are visibly smaller than tokenization (9k+).\par}
{\pard \ql \f0 \sa180 \li360 \fi-360 \bullet \tx360\tab Makes bottlenecks obvious, reinforcing the scaling plan\u8217's focus on these stages.\sa180\par}
{\pard \ql \f0 \sa180 \li0 \fi0 \outlinelevel2 \b \fs28 Metrics Summary Table\par}
{\pard \ql \f0 \sa180 \li360 \fi-360 \bullet \tx360\tab Consolidates totals, rates, and counts so reviewers can validate the textual summary at a glance.\par}
{\pard \ql \f0 \sa180 \li360 \fi-360 \bullet \tx360\tab Acts as a quick QA gate without opening raw CSV files.\sa180\par}
