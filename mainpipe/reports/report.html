<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <title>Mainpipe Pipeline Report</title>
    <style>
      body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
        max-width: 900px;
        margin: 2rem auto;
        padding: 0 1.5rem 4rem;
        line-height: 1.6;
        color: #1f2933;
        background: #ffffff;
      }
      h1, h2, h3, h4, h5, h6 {
        color: #0f172a;
        margin-top: 2.4rem;
      }
      code {
        font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
        background: #f4f4f5;
        padding: 0.2rem 0.4rem;
        border-radius: 4px;
      }
      pre > code {
        display: block;
        padding: 1rem;
        overflow-x: auto;
      }
      img {
        max-width: 100%;
        display: block;
        margin: 1rem auto;
      }
      table {
        border-collapse: collapse;
        width: 100%;
        margin: 1.5rem 0;
      }
      table, th, td {
        border: 1px solid #cbd5f5;
      }
      th, td {
        padding: 0.6rem;
        text-align: left;
      }
      a {
        color: #2563eb;
      }
      .generated-note {
        margin-top: 3rem;
        font-size: 0.9rem;
        color: #475569;
      }
    </style>
  </head>
  <body>
    <h1 id="mainpipe-pipeline-report">Mainpipe Pipeline Report</h1>
<hr>
<p><strong>Prepared By:</strong> Sally Lan</p>
<h2 id="table-of-contents">Table of Contents</h2>
<hr>
<ul>
<li><a href="#executive-summary">Executive Summary</a></li>
<li><a href="#stage-1--data-acquisition--language-filtering">Stage 1 — Data acquisition + language filtering</a></li>
<li><a href="#stage-2--text-cleaning--normalisation">Stage 2 — Text cleaning &amp; normalisation</a></li>
<li><a href="#stage-3--safety-filtering-pii--toxicity">Stage 3 — Safety filtering (PII &amp; toxicity)</a></li>
<li><a href="#stage-4--deduplication">Stage 4 — Deduplication</a></li>
<li><a href="#stage-5--tokenisation">Stage 5 — Tokenisation</a></li>
<li><a href="#stage-51--compute-perplexity">Stage 5.1 — Compute perplexity</a></li>
<li><a href="#stage-6--inspectability--visualization-analysis">Stage 6 — Inspectability &amp; Visualization Analysis</a></li>
<li><a href="#stage-7--conceptual-plan-for-scaling">Stage 7 — Conceptual plan for scaling</a></li>
<li><a href="#final-section--main-takeaways-of-performance">Final Section — Main Takeaways of Performance</a></li>
<li><a href="#appendix--visualization-insights">Appendix — Visualization Insights</a></li>
</ul>
<h2 id="executive-summary">Executive Summary</h2>
<hr>
<h3 id="purpose">Purpose</h3>
<ul>
<li>Transform multi-language raw text into safe, deduplicated, tokenized English corpora suitable for LLM fine-tuning.</li>
</ul>
<h3 id="key-outcomes">Key outcomes</h3>
<ul>
<li>183,052 kept samples; safety filters removed 7,379 risky items; MinHash dedup trimmed 19.7% duplicates without quality loss.</li>
</ul>
<h3 id="high-level-metrics">High-level metrics</h3>
<ul>
<li>Average kept sample = 1,414 chars / 312 tokens.</li>
<li>PII hit rate 2.7% and toxicity rate 0.41%.</li>
<li>Perplexity mean 41.8 (median 30.6).</li>
<li>Throughput peaks at 9,081 samples/sec during tokenization.</li>
</ul>
<h2 id="stage-1-data-acquisition-language-filtering">Stage 1 — Data acquisition + language filtering</h2>
<hr>
<h3 id="method-used">Method used</h3>
<ul>
<li><code>pipeline.loader.load_jsonl</code> plus <code>langdetect</code> enforcing <code>target_language=en</code>.</li>
</ul>
<h3 id="why">Why</h3>
<ul>
<li>Maintain a monolingual corpus to prevent context mixing that destabilizes downstream instruction-following.</li>
</ul>
<h3 id="results-metrics">Results / metrics</h3>
<ul>
<li>244,559 language-vetted samples loaded.</li>
<li>English represents 90.8% of detections with German (3.8%) and Spanish (3.2%) as the largest stragglers, highlighting stable upstream sourcing.</li>
</ul>
<h3 id="limitations">Limitations</h3>
<ul>
<li><code>langdetect</code> exposes only hard labels, preventing confidence-aware triage for borderline sentences.</li>
</ul>
<h3 id="future-improvements">Future improvements</h3>
<ul>
<li>Replace detector with FastText or langid to unlock the planned Tier 2 “summarize + re-evaluate” workflow.</li>
</ul>
<h3 id="implementation-insight">Implementation &amp; insight</h3>
<ul>
<li>Language mix skew (long English tail with minor European languages) indicates crawl consistency but also shows potential for optional multilingual branches without retooling loaders.</li>
</ul>
<h2 id="stage-2-text-cleaning-normalisation">Stage 2 — Text cleaning &amp; normalisation</h2>
<hr>
<h3 id="method-used_1">Method used</h3>
<ul>
<li><code>pipeline.cleaner.clean_samples</code> applying HTML stripping, whitespace normalization, and character-length bounds of 50–100k.</li>
</ul>
<h3 id="why_1">Why</h3>
<ul>
<li>Remove boilerplate markup, normalize formatting, and constrain documents to sizes that tokenize reliably.</li>
</ul>
<h3 id="results-metrics_1">Results / metrics</h3>
<ul>
<li>7,431 documents dropped (99.7% due to being &lt;50 chars), leaving 237,128 samples—evidence that most raw inputs already provide sufficient context length.</li>
</ul>
<h3 id="limitations_1">Limitations</h3>
<ul>
<li>Binary cutoffs discard concise but high-signal snippets and do not split ultra-long documents into shards.</li>
</ul>
<h3 id="future-improvements_1">Future improvements</h3>
<ul>
<li>Add adaptive chunking for long content and summarization for short-but-useful notes.</li>
</ul>
<h3 id="implementation-insight_1">Implementation &amp; insight</h3>
<ul>
<li>Cleaning drop distribution (short &gt; long by 322×) shows upstream scraping already curbs web noise; tuning the minimum threshold could rescue niche datasets without affecting quality.</li>
</ul>
<h2 id="stage-3-safety-filtering-pii-toxicity">Stage 3 — Safety filtering (PII &amp; toxicity)</h2>
<hr>
<h3 id="method-used_2">Method used</h3>
<ul>
<li>Deterministic heuristic toxicity scoring (blocklists + uppercase/symbol/repetition penalties + obfuscated-profanity regex) paired with regex-based PII detection for EMAIL, PHONE, CREDIT_CARD, SSN, IP.</li>
</ul>
<h3 id="why_2">Why</h3>
<ul>
<li>Guarantee explainable, reproducible safety decisions without GPU inference dependencies.</li>
</ul>
<h3 id="results-metrics_2">Results / metrics</h3>
<ul>
<li>237,128 entries scanned (768-char window).</li>
<li>Heuristics flagged 971 toxic samples (0.41%).</li>
<li>PII detectors removed 6,408 records (2.7%), demonstrating that privacy risks dominate the safety tail.</li>
</ul>
<h3 id="limitations_2">Limitations</h3>
<ul>
<li>Rule sets miss nuanced harassment and subtle PII formats; regex false positives (e.g., code blocks) still waste reviewer time.</li>
</ul>
<h3 id="future-improvements_2">Future improvements</h3>
<ul>
<li>Layer detoxify-like ML scorers and NER-driven PII detectors with confidence thresholds to reduce both misses and false alarms.</li>
</ul>
<h3 id="implementation-insight_2">Implementation &amp; insight</h3>
<ul>
<li>Phone (2,984 hits) and email (2,844 hits) patterns account for 89% of PII drops, while SSNs are rare (7 hits); PII hits overall are 6.6× more frequent than toxicity hits, so the next investment should target privacy detection breadth before advanced toxicity modeling.</li>
</ul>
<h2 id="stage-4-deduplication">Stage 4 — Deduplication</h2>
<hr>
<h3 id="method-used_3">Method used</h3>
<ul>
<li>MinHash LSH (128 permutations, 0.9 Jaccard threshold) via <code>pipeline.deduplicator.deduplicate</code>.</li>
</ul>
<h3 id="why_3">Why</h3>
<ul>
<li>Remove repeated documents to minimize memorization and licensing exposure.</li>
</ul>
<h3 id="results-metrics_3">Results / metrics</h3>
<ul>
<li>45,183 duplicates removed (19.7% duplicate rate), leaving ~184k unique entries—indicative of partially overlapping crawls.</li>
</ul>
<h3 id="limitations_3">Limitations</h3>
<ul>
<li>LSH accuracy drops for short samples and scales poorly with corpus size, risking slowdowns at &gt;1M documents.</li>
</ul>
<h3 id="future-improvements_3">Future improvements</h3>
<ul>
<li>Introduce streaming dedup with hashed fingerprints or partitioned shingles to bound memory and CPU.</li>
</ul>
<h3 id="implementation-insight_3">Implementation &amp; insight</h3>
<ul>
<li>Duplicate share closely mirrors historic web crawl overlap, so upstream dedup (per domain) could reclaim ~20% pipeline time before data even enters MinHash.</li>
</ul>
<h2 id="stage-5-tokenisation">Stage 5 — Tokenisation</h2>
<hr>
<h3 id="method-used_4">Method used</h3>
<ul>
<li><code>pipeline.tokenizer.tokenize_samples</code> with tiktoken <code>cl100k_base</code>, enforcing 10–8,192 token bounds.</li>
</ul>
<h3 id="why_4">Why</h3>
<ul>
<li>Align dataset with typical OpenAI context windows and avoid fragments that fail to train sequence models.</li>
</ul>
<h3 id="results-metrics_4">Results / metrics</h3>
<ul>
<li>Average kept sample = 312 tokens (median 168).</li>
<li>1,514 records dropped (1,000 too short, 514 too long) leaving 183,052 training-ready rows.</li>
<li>Length histogram skews toward 150–400 tokens, validating the cleaning thresholds.</li>
</ul>
<h3 id="limitations_4">Limitations</h3>
<ul>
<li>Uniform token limits ignore downstream model variation (4k vs 16k contexts) and do not create curriculum bins.</li>
</ul>
<h3 id="future-improvements_4">Future improvements</h3>
<ul>
<li>Parameterize token bounds per export profile and optionally emit stratified shards.</li>
</ul>
<h3 id="implementation-insight_4">Implementation &amp; insight</h3>
<ul>
<li>The token-length hump around 200 tokens suggests we can raise the minimum from 10 to ~25 without losing coverage, improving effective context per sample.</li>
</ul>
<h2 id="stage-51-compute-perplexity">Stage 5.1 — Compute perplexity</h2>
<hr>
<h3 id="method-used_5">Method used</h3>
<ul>
<li><code>pipeline.perplexity.compute_perplexity</code> sampling 1,000 kept documents and running GPT-2 small inference.</li>
</ul>
<h3 id="why_5">Why</h3>
<ul>
<li>Track linguistic quality and detect drift in a lightweight, repeatable manner.</li>
</ul>
<h3 id="results-metrics_5">Results / metrics</h3>
<ul>
<li>Mean perplexity 41.8 (median 30.6, std 50.7, min 2.2, max 949).</li>
<li>Long upper tail indicates sparse low-quality spans despite strong median quality.</li>
</ul>
<h3 id="limitations_5">Limitations</h3>
<ul>
<li>Single English GPT-2 checkpoint underrepresents domain-specific or multilingual issues; the 1k sample may miss rare regressions.</li>
</ul>
<h3 id="future-improvements_5">Future improvements</h3>
<ul>
<li>Add domain-tuned checkpoints, dynamic sample sizing, and percentile-based gating before export.</li>
</ul>
<h3 id="implementation-insight_5">Implementation &amp; insight</h3>
<ul>
<li>75% of evaluated docs fall below perplexity 47.9, so gating at 60 would catch only anomalous spikes while leaving the main distribution untouched.</li>
</ul>
<h2 id="stage-6-inspectability-visualization-analysis">Stage 6 — Inspectability &amp; Visualization Analysis</h2>
<hr>
<h3 id="method-used_6">Method used</h3>
<ul>
<li><code>pipeline.inspector.inspect</code> compiles JSON/CSV summaries and renders Matplotlib charts for token lengths, safety drops, PII types, throughput, and language mix.</li>
</ul>
<h3 id="why_6">Why</h3>
<ul>
<li>Give reviewers a dashboard-equivalent artifact directly in the repo.</li>
</ul>
<h3 id="results-metrics_6">Results / metrics</h3>
<ul>
<li>Token-length plot confirms the 200-token mode.</li>
<li>Safety charts show PII removal dominating toxicity.</li>
<li>Throughput visualization highlights load/dedup bottlenecks (each under 650 samples/sec).</li>
</ul>
<h3 id="limitations_6">Limitations</h3>
<ul>
<li>Static PNGs require reruns for new slices, and there is no drill-down into sample-level evidence.</li>
</ul>
<h3 id="future-improvements_6">Future improvements</h3>
<ul>
<li>Pipe metrics into DuckDB/Parquet and layer an interactive notebook or dashboard for slice-and-dice QA.</li>
</ul>
<h3 id="implementation-insight_6">Implementation &amp; insight</h3>
<ul>
<li>Visual overlays make it obvious that dedup time dwarfs other stages despite modest drop counts, reinforcing the need for architectural change rather than mere parameter tuning.</li>
</ul>
<h2 id="stage-7-conceptual-plan-for-scaling">Stage 7 — Conceptual plan for scaling</h2>
<hr>
<h3 id="method-used-architecture-choices">Method used / architecture choices</h3>
<ul>
<li>Near-term multiprocessing and streaming; mid-term Ray dataflow with cloud storage; long-term Spark-style distributed tokenization plus checkpointed incremental runs.</li>
</ul>
<h3 id="why_7">Why</h3>
<ul>
<li>Current bottlenecks (load at 492 samples/sec, dedup at 616 samples/sec, perplexity at 3 samples/sec) limit throughput as corpora grow toward billions of tokens.</li>
</ul>
<h3 id="results-metrics-expected">Results / metrics (expected)</h3>
<ul>
<li>Ray should sustain ≥2,500 samples/sec per worker for language + safety.</li>
<li>Parquet/DuckDB export minimizes downstream I/O.</li>
<li>Spark integration targets petabyte-scale ingestion with fault tolerance.</li>
</ul>
<h3 id="limitations-risks">Limitations / risks</h3>
<ul>
<li>Distributed dedup complicates coordination; GPU tokenization requires specialized hardware; remote storage adds auth and latency failure modes.</li>
</ul>
<h3 id="future-improvements-migration-plan">Future improvements / migration plan</h3>
<ul>
<li>Pilot Ray for language/safety, adopt S3/GCS streaming connectors, then roll Spark once sharding and checkpointing stabilize.</li>
</ul>
<h3 id="monitoring-failure-modes">Monitoring &amp; failure modes</h3>
<ul>
<li>Continue per-stage timers (<code>throughput_metrics.stages</code>), add executor health probes, and alert on slow shards or stalled checkpoints.</li>
</ul>
<h2 id="final-section-main-takeaways-of-performance">Final Section — Main Takeaways of Performance</h2>
<hr>
<h3 id="top-performance-highlights">Top performance highlights</h3>
<ul>
<li>Cleaning + tokenization sustain multi-thousand samples/sec.</li>
<li>Deterministic safety maintains reviewer trust.</li>
<li>Inspector artifacts provide immediate QA context without external dashboards.</li>
</ul>
<h3 id="key-bottlenecks-identified">Key bottlenecks identified</h3>
<ul>
<li>JSONL loading and MinHash dedup dominate wall-clock time.</li>
<li>Perplexity inference lags at 3.1 samples/sec.</li>
</ul>
<h3 id="priority-next-steps">Priority next steps</h3>
<ul>
<li>Parallelize load/dedup via Ray or Spark.</li>
<li>Adopt probability-aware language detection.</li>
<li>Layer ML safety assistance.</li>
<li>Persist metrics in DuckDB/Parquet to enable interactive QA and alerting.</li>
</ul>
<h2 id="appendix-visualization-insights">Appendix — Visualization Insights</h2>
<hr>
<h3 id="char-length-histogram">Char Length Histogram</h3>
<p><img alt="Char Length Histogram" src="visualizations/char_length_histogram.png"></p>
<ul>
<li>
<p>Most kept samples fall between 500 and 2,000 characters, validating the 50–100k cleaning bounds.</p>
</li>
<li>
<p>Heavy density in this band confirms documents retain substantial narrative context for training.</p>
</li>
</ul>
<h3 id="token-length-histogram">Token Length Histogram</h3>
<p><img alt="Token Length Histogram" src="visualizations/token_length_histogram.png"></p>
<ul>
<li>
<p>Peak near 200 tokens mirrors the character histogram, showing mid-length contexts dominate.</p>
</li>
<li>
<p>Supports raising the minimum token threshold (e.g., from 10 to ~25) without losing coverage.</p>
</li>
</ul>
<h3 id="cleaning-drop-reasons">Cleaning Drop Reasons</h3>
<p><img alt="Cleaning Drop Reasons" src="visualizations/cleaning_drop_reasons.png"></p>
<ul>
<li>
<p>“Too short” removals dwarf “too long,” proving upstream scraping already curbs oversized docs.</p>
</li>
<li>
<p>Residual short snippets highlight an opportunity for summarization instead of full drops.</p>
</li>
</ul>
<h3 id="language-distribution">Language Distribution</h3>
<p><img alt="Language Distribution" src="visualizations/language_distribution.png"></p>
<ul>
<li>
<p>English holds a dominant share with small German/Spanish/French slices, matching loader stats.</p>
</li>
<li>
<p>The long tail suggests we can optionally branch multilingual processing without retooling ingestion.</p>
</li>
</ul>
<h3 id="pii-removals">PII Removals</h3>
<p><img alt="PII Removals" src="visualizations/pii_removals.png"></p>
<ul>
<li>
<p>Phone numbers (2,984) and email addresses (2,844) represent ~89% of PII hits.</p>
</li>
<li>
<p>Aligns with prioritizing communication-identifier detection before rarer entities like SSNs (7 hits).</p>
</li>
</ul>
<h3 id="toxic-removals">Toxic Removals</h3>
<p><img alt="Toxic Removals" src="visualizations/toxic_removals.png"></p>
<ul>
<li>
<p>Toxicity drops are comparatively low but show noticeable spikes tied to uppercase/obfuscation-heavy texts.</p>
</li>
<li>
<p>Confirms deterministic heuristics focus on high-confidence rule violations.</p>
</li>
</ul>
<h3 id="perplexity-distribution">Perplexity Distribution</h3>
<p><img alt="Perplexity Distribution" src="visualizations/perplexity_distribution.png"></p>
<ul>
<li>
<p>Median lies near 30 with a long tail extending beyond 900, evidencing sparse low-quality spans.</p>
</li>
<li>
<p>Supports gating exports near perplexity 60 to capture outliers without affecting the bulk.</p>
</li>
</ul>
<h3 id="throughput-overview">Throughput Overview</h3>
<p><img alt="Throughput Overview" src="visualizations/throughput_overview.png"></p>
<ul>
<li>
<p><code>load_jsonl</code> (~492 samples/sec) and deduplication (~616 samples/sec) bars are visibly smaller than tokenization (9k+).</p>
</li>
<li>
<p>Makes bottlenecks obvious, reinforcing the scaling plan’s focus on these stages.</p>
</li>
</ul>
<h3 id="metrics-summary-table">Metrics Summary Table</h3>
<p><img alt="Metrics Summary Table" src="visualizations/metrics_summary_table.png"></p>
<ul>
<li>
<p>Consolidates totals, rates, and counts so reviewers can validate the textual summary at a glance.</p>
</li>
<li>
<p>Acts as a quick QA gate without opening raw CSV files.</p>
</li>
</ul>
    <p class="generated-note">Generated on 2025-11-19 13:02 UTC</p>
  </body>
</html>
